# -*- coding: utf-8 -*-
"""Características da Maxila - Sexo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r1hvrRU4OQwRACNf4bnDYXQA_4V0b3sQ

# **Library and packages**
"""

import pandas as pd
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
from sklearn.preprocessing import LabelBinarizer

"""# **Dataset**"""

file_path = '/content/dataset5.xlsx'
df = pd.read_excel(file_path)

df.columns

"""# **Data preprocessing**"""

colunas = [ 'Gender', 'PTM-PTM', 'J-J', 'Ln-Ln', 'Mol-Mol',
       'Pre-Pre', 'ANS-PNS','NFW']


df = df.loc[:, colunas]

coluna_grupo = df.pop('Gender')
df.insert(0, 'Gender', coluna_grupo)

df.info()
df.columns

"""# **Model building**

**Data splitting**
"""

X = df.iloc[:,1:]
y = df.iloc[:,0]

RANDOM_STATE = 24
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=RANDOM_STATE)

print(f"Train data shape of X = {X_train.shape} and Y = {y_train.shape}")
print(f"Test data shape of X = {X_test.shape} and Y = {y_test.shape}")

##Data normalization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

y_train = y_train.astype('category')
y_test = y_test.astype('category')

# K-Folds
fold = 5

"""**GRADIENT BOOSTING**"""

param_grid = {
    'n_estimators': [2000],
    'learning_rate': [0.001, 0.01, 0.1],
    'max_depth': [3, 5, 7],
    'criterion': ['friedman_mse', 'mse'],
    'loss': ['deviance', 'exponential']
}
grd_boost = GradientBoostingClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=grd_boost, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_grd_boost = grid_search.best_estimator_
best_grd_boost.fit(X_train, y_train)
y_pred_test_gb = best_grd_boost.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred_test_gb)
conf_matrix = confusion_matrix(y_test, y_pred_test_gb)
accuracy_gb = accuracy_score(y_test, y_pred_test_gb)
report = classification_report(y_test, y_pred_test_gb)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_gb)
print("Classification Report (Test Data):\n", report)
kf_gb = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_gb = cross_val_predict(best_grd_boost, X_train, y_train, cv=kf_gb)
conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_gb)
accuracy_cv_gb = accuracy_score(y_train, y_pred_cv_gb)
report_cv = classification_report(y_train, y_pred_cv_gb)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_gb)
print("Classification Report (Cross-Validation):\n", report_cv)

#Calculation of ROC Curve metrics and graph plotting
best_grd_boost.fit(X_train, y_train)
y_prob_gb = best_grd_boost.predict_proba(X_test)
label_binarizer = LabelBinarizer()
y_test_bin = label_binarizer.fit_transform(y_test)

fpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test_bin, y_prob_gb[:,1],pos_label=1)

# roc curve for tpr = fpr
random_probs = [0 for i in range(len(y_test_bin))]
p_fpr, p_tpr, _ = roc_curve(y_test_bin, random_probs, pos_label=1)


# auc scores
roc_auc_gb = roc_auc_score(y_test_bin, y_prob_gb[:,1])
print(roc_auc_gb)


# matplotlib
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr_gb, tpr_gb, linestyle='--',color='orange', label=f'ROC Curve (AUC = {roc_auc_gb:.2f})')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

##FEATURE IMPORTANCE
X = df.drop('Gender', axis=1)
features=[]
for columns in X.columns:
    features.append(columns)
imp_features = best_grd_boost.feature_importances_
importances = best_grd_boost.feature_importances_

feature_importance_gb = pd.DataFrame({'Feature': features, 'Importance': importances})

feature_importance_sorted = feature_importance_gb.sort_values('Importance', ascending=True)
colors = plt.cm.Greens(np.linspace(0.2, 1, len(feature_importance_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

plt.savefig('figimpKNN.jpg', dpi=300, bbox_inches='tight')

"""**REGRESSÃO LOGÍSTICA**"""

param_grid = {

    'C': [0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 5, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'max_iter':[50, 100, 300, 500, 1000],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'l1_ratio': [0.2, 0.4, 0.6, 0.8]
}
log_reg = LogisticRegression(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_log_reg = grid_search.best_estimator_
best_log_reg.fit(X_train, y_train)
y_pred_test_lr = best_log_reg.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred_test_lr)
accuracy_lr = accuracy_score(y_test, y_pred_test_lr)
report = classification_report(y_test, y_pred_test_lr)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_lr)
print("Classification Report (Test Data):\n", report)
kf_lr = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_lr = cross_val_predict(best_log_reg, X_train, y_train, cv=kf_lr)
conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_lr)
accuracy_cv_lr = accuracy_score(y_train, y_pred_cv_lr)
report_cv = classification_report(y_train, y_pred_cv_lr)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_lr)
print("Classification Report (Cross-Validation):\n", report_cv)

#Calculation of ROC Curve metrics and graph plotting
best_log_reg.fit(X_train, y_train)
y_prob_lr = best_log_reg.predict_proba(X_test)

fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test_bin, y_prob_lr[:,1],pos_label=1)

# auc scores
roc_auc_lr = roc_auc_score(y_test_bin, y_prob_lr[:,1])
print(roc_auc_lr)


# matplotlib
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr_lr, tpr_lr, linestyle='--',color='green', label=f'ROC Curve (AUC = {roc_auc_lr:.2f})')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

#FEATURE IMPORTANCE
model = best_log_reg
coefficients = model.coef_[0]
feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=True)
colors = plt.cm.Greens(np.linspace(0.2, 1, len(feature_importance_lr)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_lr['Feature'], feature_importance_lr['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# SVM"""

param_grid = {
    'C': [0.001, 0.1, 0.9, 1.5, 2.3,23, 50, 100],
    'kernel': ['linear','rbf'],
    'gamma': ['auto','scale']
}

svm = SVC(probability=True, random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_svm = grid_search.best_estimator_
best_svm.fit(X_train, y_train)
y_pred_test_svm = best_svm.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred_test_svm)
accuracy_svm = accuracy_score(y_test, y_pred_test_svm)
report = classification_report(y_test, y_pred_test_svm)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_svm)
print("Classification Report (Test Data):\n", report)

kf_svm = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_svm = cross_val_predict(best_svm, X_train, y_train, cv=kf_svm)
conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_svm)
accuracy_cv_svm = accuracy_score(y_train, y_pred_cv_svm)
report_cv = classification_report(y_train, y_pred_cv_svm)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_svm)
print("Classification Report (Cross-Validation):\n", report_cv)

#Calculation of ROC Curve metrics and graph plotting
best_svm.fit(X_train, y_train)
y_prob_svm = best_svm.predict_proba(X_test)

fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test_bin, y_prob_svm[:,1],pos_label=1)

# auc scores
roc_auc_svm = roc_auc_score(y_test_bin, y_prob_svm[:,1])
print(roc_auc_svm)


# matplotlib
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr_svm, tpr_svm, linestyle='--',color='red', label=f'ROC Curve (AUC = {roc_auc_svm:.2f})')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

"""# KNN"""

param_grid = {
   'n_neighbors': [1, 3, 5, 7, 10, 15, 100, 1000],
    'weights': ['uniform', 'distance'],
    'p': [0.001, 0.1, 1, 3, 5, 7, 10, 15, 100, 1000],
    'leaf_size': [0.001, 0.1, 1, 3, 5, 7, 10, 15, 100, 1000]
}

knn = KNeighborsClassifier()
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_knn = grid_search.best_estimator_
best_knn.fit(X_train, y_train)
y_pred_test_knn = best_knn.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred_test_knn)
accuracy_knn = accuracy_score(y_test, y_pred_test_knn)
report = classification_report(y_test, y_pred_test_knn)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_knn)
print("Classification Report (Test Data):\n", report)
kf_knn = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_knn = cross_val_predict(best_knn, X_train, y_train, cv=kf_knn)

conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_knn)
accuracy_cv_knn = accuracy_score(y_train, y_pred_cv_knn)
report_cv = classification_report(y_train, y_pred_cv_knn)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_knn)
print("Classification Report (Cross-Validation):\n", report_cv)

#Calculation of ROC Curve metrics and graph plotting
best_knn.fit(X_train, y_train)
y_prob_knn = best_knn.predict_proba(X_test)

fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test_bin, y_prob_knn[:,1],pos_label=1)

# auc scores
roc_auc_knn = roc_auc_score(y_test_bin, y_prob_knn[:,1])
print(roc_auc_knn)


# matplotlib
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr_knn, tpr_knn, linestyle='--',color='deeppink', label=f'ROC Curve (AUC = {roc_auc_knn:.2f})')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

"""# MLP CLASSIFIER"""

param_grid = {
    'hidden_layer_sizes': [10, 100, 1000],
    'alpha': [ 0.01, 0.1, 1.0],
    'learning_rate_init': [0.01, 0.1, 1],
    'activation': ['relu', 'logistic', 'tanh'],
    'max_iter': [50, 100, 1000],
    'solver': ['lbfgs', 'sgd', 'adam']
}

mlp_clf = MLPClassifier(random_state=42)
grid_search = GridSearchCV(estimator=mlp_clf, param_grid=param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

print("Melhores Parâmetros Encontrados:")
print(grid_search.best_params_)
best_mlp_clf = grid_search.best_estimator_
y_pred_mlp_test = best_mlp_clf.predict(X_test)
conf_matrix_mlp_test = confusion_matrix(y_test, y_pred_mlp_test)
accuracy_mlp_test = accuracy_score(y_test, y_pred_mlp_test)
report_mlp_test = classification_report(y_test, y_pred_mlp_test)

print("Confusion Matrix (Test Data):\n", conf_matrix_mlp_test)
print("Accuracy (Test Data):", accuracy_mlp_test)
print("Classification Report (Test Data):\n", report_mlp_test)
kf_mlp = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_mlp_cv = cross_val_predict(best_mlp_clf, X_train, y_train, cv=kf_mlp)
conf_matrix_mlp_cv = confusion_matrix(y_train, y_pred_mlp_cv)
accuracy_mlp_cv = accuracy_score(y_train, y_pred_mlp_cv)
report_mlp_cv = classification_report(y_train, y_pred_mlp_cv)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_mlp_cv)
print("Cross-Validation Accuracy:", accuracy_mlp_cv)
print("Classification Report (Cross-Validation):\n", report_mlp_cv)

#Calculation of ROC Curve metrics and graph plotting
best_mlp_clf.fit(X_train, y_train)
y_prob_mlp = best_mlp_clf.predict_proba(X_test)

fpr_mlp, tpr_mlp, thresholds_mlp = roc_curve(y_test_bin, y_prob_mlp[:,1],pos_label=1)

# auc scores
roc_auc_mlp = roc_auc_score(y_test_bin, y_prob_mlp[:,1])
print(roc_auc_mlp)


# matplotlib
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr_mlp, tpr_mlp, linestyle='--',color='indigo', label=f'ROC Curve (AUC = {roc_auc_mlp:.2f})')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

"""# DECISION TREE"""

param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'max_depth': [None, 5, 10, 15],
}

tree_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=tree_clf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_tree_clf = grid_search.best_estimator_
best_tree_clf.fit(X_train, y_train)
y_pred_test_dt = best_tree_clf.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred_test_dt)
accuracy_dt = accuracy_score(y_test, y_pred_test_dt)
report = classification_report(y_test, y_pred_test_dt)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_dt)
print("Classification Report (Test Data):\n", report)
kf_dt = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_dt = cross_val_predict(best_tree_clf, X_train, y_train, cv=kf_dt)
conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_dt)
accuracy_cv_dt = accuracy_score(y_train, y_pred_cv_dt)
report_cv = classification_report(y_train, y_pred_cv_dt)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_dt)
print("Classification Report (Cross-Validation):\n", report_cv)

#Calculation of ROC Curve metrics and graph plotting


best_tree_clf.fit(X_train, y_train)
y_prob_tree = best_tree_clf.predict_proba(X_test)
label_binarizer = LabelBinarizer()
y_test_bin = label_binarizer.fit_transform(y_test)


fpr_tree, tpr_tree, thresholds_tree = metrics.roc_curve(y_test_bin, y_prob_tree[:,1],pos_label=1)

# auc scores
roc_auc_tree = roc_auc_score(y_test_bin, y_prob_tree[:,1])
print(roc_auc_tree)


# roc curve for tpr = fpr
random_probs = [0 for i in range(len(y_test_bin))]
p_fpr, p_tpr, _ = roc_curve(y_test_bin, random_probs, pos_label=1)


# matplotlib
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr_tree, tpr_tree, linestyle='--',color='steelblue', label=f'ROC Curve (AUC = {roc_auc_tree:.2f})')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

#FEATURE IMPORTANCE
X_2 = []
X_2
features_2 =[]
imp_features_dt2 = []
df_imp_features_dt2 = []
X_2 = df.drop('Gender', axis=1)
features_2 = []

for column in X_2.columns:
    features_2.append(column)

imp_features_dt2 = best_tree_clf.feature_importances_
df_imp_features_dt2 = pd.DataFrame({"features": features_2, "weights": imp_features_dt2})
df_imp_features_dt2_sorted = df_imp_features_dt2.sort_values(by='weights', ascending=True)
plt.figure(figsize=(10, 6))
colors = plt.cm.Greens(np.linspace(0.2, 1, len(df_imp_features_dt2_sorted)))
bars = plt.barh(df_imp_features_dt2_sorted['features'], df_imp_features_dt2_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# RANDOM FOREST"""

param_grid = {
    'n_estimators': [5, 50, 200],
    'max_depth': [0, 10, 20],
    'min_samples_split': [2, 10, 15],
    'min_samples_leaf': [1, 4, 6],
    'max_features': ['auto', 'sqrt'],
    'criterion': ['gini', 'entropy']
}

forest_clf = RandomForestClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=forest_clf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_forest_clf = grid_search.best_estimator_
best_forest_clf.fit(X_train, y_train)


y_pred_test_rf = best_forest_clf.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred_test_rf)

accuracy_rf = accuracy_score(y_test, y_pred_test_rf)
report = classification_report(y_test, y_pred_test_rf)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_rf)
print("Classification Report (Test Data):\n", report)

kf_rf = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_rf = cross_val_predict(best_forest_clf, X_train, y_train, cv=kf_rf)

conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_rf)
accuracy_cv_rf = accuracy_score(y_train, y_pred_cv_rf)
report_cv = classification_report(y_train, y_pred_cv_rf)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_rf)
print("Classification Report (Cross-Validation):\n", report_cv)

#Calculation of ROC Curve metrics and graph plotting
best_forest_clf.fit(X_train, y_train)
y_prob_forest = best_forest_clf.predict_proba(X_test)

fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_test_bin, y_prob_forest[:,1],pos_label=1)

# auc scores
roc_auc_forest = roc_auc_score(y_test_bin, y_prob_forest[:,1])
print(roc_auc_forest)


# matplotlib
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr_forest, tpr_forest, linestyle='--',color='maroon', label=f'ROC Curve (AUC = {roc_auc_forest:.2f})')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

#FEATURE IMPORTANCE

X_3 = df.drop('Gender', axis=1)
features_3 = []

for column in X_3.columns:
    features_3.append(column)

imp_features_dt3 = best_forest_clf.feature_importances_
df_imp_features_dt3 = pd.DataFrame({"features": features_3, "weights": imp_features_dt3})
df_imp_features_dt3_sorted = df_imp_features_dt3.sort_values(by='weights', ascending=True)
plt.figure(figsize=(10, 6))
colors = plt.cm.Greens(np.linspace(0.2, 1, len(df_imp_features_dt3_sorted)))
bars = plt.barh(df_imp_features_dt3_sorted['features'], df_imp_features_dt3_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# Plot-ROC"""

# ROC Curve
plt.figure(figsize=(10, 8))  # Definir o tamanho da figura

plt.plot(fpr_lr, tpr_lr, color='red', lw=2, linestyle='--', label='LR ROC curve (area = %0.2f)' % roc_auc_lr)
plt.plot(fpr_forest, tpr_forest, color='deeppink',linestyle='--', lw=2, label='RF ROC curve (area = %0.2f)' % roc_auc_forest)
plt.plot(fpr_tree, tpr_tree, color='gray', lw=2,linestyle='--', label='DT ROC curve (area = %0.2f)' % roc_auc_tree)
plt.plot(fpr_svm, tpr_svm, color='green', lw=2,linestyle='--', label='SVM ROC curve (area = %0.2f)' % roc_auc_svm)
plt.plot(fpr_mlp, tpr_mlp, color='purple', lw=2,linestyle='--', label='MLP ROC curve (area = %0.2f)' % roc_auc_mlp)
plt.plot(fpr_gb, tpr_gb, color='orange', lw=2,linestyle='--', label='GB ROC curve (area = %0.2f)' % roc_auc_gb)
plt.plot(fpr_knn, tpr_knn, color='blue', lw=2,linestyle='--', label='KNN ROC curve (area = %0.2f)' % roc_auc_knn)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='-')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1- Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.grid(True)  # Adicionar linhas de grade

# Salvar a figura em alta resolução
plt.savefig('roc_curve.jpg', dpi=300, bbox_inches='tight')

plt.show()